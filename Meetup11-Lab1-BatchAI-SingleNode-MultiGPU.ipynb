{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python TensorFlow and CNTK GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This example uses the MNIST dataset to demonstrate how to train a convolutional neural network (CNN) on a GPU cluster. This recipe is running on a single node.\n",
    "\n",
    "## Details\n",
    "\n",
    "- For demonstration purposes, MNIST dataset and ConvNet_MNIST.py will be deployed at Azure File Share;\n",
    "- Standard output of the job and the model will be stored on Azure File Share;\n",
    "- MNIST dataset (http://yann.lecun.com/exdb/mnist/) has been preprocessed by usign install_mnist.py available at https://batchaisamples.blob.core.windows.net/samples/mnist_dataset.zip?st=2017-09-29T18%3A29%3A00Z&se=2099-12-31T08%3A00%3A00Z&sp=rl&sv=2016-05-31&sr=c&sig=PmhL%2BYnYAyNTZr1DM2JySvrI12e%2F4wZNIwCtf7TRI%2BM%3D.\n",
    "- The original CNTK example (https://github.com/Microsoft/CNTK/blob/master/Examples/Image/Classification/ConvNet/Python/ConvNet_MNIST.py) has been modified to accept CNTK dataset and model locations via command line arguments and available here [ConvNet_MNIST.py](/recipes/CNTK/Python/CNTK-GPU-Python/ConvNet_MNIST.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Install Dependencies and Create Configuration file.\n",
    "```\n",
    "> ssh sshuser@YOUR.VM.IP.ADDRESS\n",
    "> az login\n",
    "> az group create --name batchai_rg  --location eastus\n",
    "> az storage account create --location eastus --name batchaipablo --resource-group batchai_rg --sku Standard_LRS\n",
    "> az storage account keys list --account-name batchaipablo --resource-group batchai_rg -o table\n",
    "> az ad sp create-for-rbac --name MyAppSvcPppl --password Passw0rd\n",
    "> az storage account keys list --account-name batchaipablo --resource-group batchai_rg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration and Create Batch AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "bfa11f00-8866-4051-bbfe-a9646e004910"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "from azure.storage.file import FileService\n",
    "import azure.mgmt.batchai.models as models\n",
    "\n",
    "# utilities.py contains helper functions\n",
    "import utilities\n",
    "\n",
    "# Resource Group\n",
    "location = 'eastus'\n",
    "resource_group = 'batchai_rg'\n",
    "\n",
    "# credentials used for authentication\n",
    "client_id = 'ec0640c7-61fa-4662-bce4-8a3e931939ac'\n",
    "#secret = '/ScxxSeAlys0E94c2zBSVNr8yjGPjRcOn9hGQiG6z/4='\n",
    "secret = 'Passw0rd'\n",
    "token_uri = 'https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47/oauth2/token'\n",
    "subscription_id = 'b1395605-1fe9-4af4-b3ff-82a4725a3791'\n",
    "\n",
    "# credentials used for storage\n",
    "storage_account_name = 'batchaipablo'\n",
    "storage_account_key = 'y59heteYEbw5nTLBB/b7rj3jUphvs2Iwslg4AsXFSb4G7ZLgJUep4AuccSmST7I3E8Zw4BaUloebK+VyKmGpog=='\n",
    "\n",
    "# specify the credentials used to remote login your GPU node\n",
    "admin_user_name = 'sshuser'\n",
    "admin_user_password = 'Passw0rd.1!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.common.credentials import ServicePrincipalCredentials\n",
    "import azure.mgmt.batchai as batchai\n",
    "import azure.mgmt.batchai.models as models\n",
    "\n",
    "creds = ServicePrincipalCredentials(client_id=client_id, secret=secret, token_uri=token_uri)\n",
    "\n",
    "client = batchai.BatchAIManagementClient(credentials=creds,subscription_id=subscription_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create File Share\n",
    "\n",
    "For this example we will create a new File Share with name `batchaisample` under your storage account.\n",
    "\n",
    "**Note** You don't need to create new file share for every cluster. We are doing this in this sample to simplify resource management for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "azure_file_share_name = 'batchaisample'\n",
    "service = FileService(storage_account_name, storage_account_key)\n",
    "service.create_share(azure_file_share_name, fail_on_exist=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Compute Cluster\n",
    "\n",
    "- For this example we will use a gpu cluster of `STANDARD_NC6` nodes. Number of nodes in the cluster is configured with `nodes_count` variable;\n",
    "- We will mount file share at folder with name `external`. Full path of this folder on a computer node will be `$AZ_BATCHAI_MOUNT_ROOT/external`;\n",
    "- We will call the cluster `nc6`;\n",
    "\n",
    "So, the cluster will have the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_file_share = 'external'\n",
    "nodes_count = 1\n",
    "cluster_name = 'nc24'\n",
    "\n",
    "volumes = models.MountVolumes(\n",
    "    azure_file_shares=[\n",
    "        models.AzureFileShareReference(\n",
    "            account_name=storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=storage_account_key),\n",
    "            azure_file_url = 'https://{0}.file.core.windows.net/{1}'.format(\n",
    "                storage_account_name, azure_file_share_name),\n",
    "            relative_mount_path=azure_file_share)\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = models.ClusterCreateParameters(\n",
    "    location=location,\n",
    "    vm_size=\"Standard_NC24\",\n",
    "    virtual_machine_configuration=models.VirtualMachineConfiguration(\n",
    "        image_reference=models.ImageReference(\n",
    "            publisher=\"microsoft-ads\",\n",
    "            offer=\"linux-data-science-vm-ubuntu\",\n",
    "            sku=\"linuxdsvmubuntu\",\n",
    "            version=\"latest\")),    \n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=admin_user_name,\n",
    "        admin_user_password=admin_user_password),\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        manual=models.ManualScaleSettings(target_node_count=nodes_count)\n",
    "    ),\n",
    "    node_setup=models.NodeSetup(\n",
    "        mount_volumes=volumes,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = client.clusters.create(resource_group, cluster_name, parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Cluster Creation\n",
    "\n",
    "Monitor the just created cluster. utilities.py contains a helper function to print out detail status of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: AllocationState.steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n"
     ]
    }
   ],
   "source": [
    "cluster = client.clusters.get(resource_group, cluster_name)\n",
    "utilities.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deploy MNIST Dataset\n",
    "\n",
    "For demonstration purposes, we will download preprocessed MNIST dataset to the current directory and upload it to file share directory named `mnist_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Extract MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset_url = 'https://batchaisamples.blob.core.windows.net/samples/mnist_dataset.zip?st=2017-09-29T18%3A29%3A00Z&se=2099-12-31T08%3A00%3A00Z&sp=rl&sv=2016-05-31&sr=c&sig=PmhL%2BYnYAyNTZr1DM2JySvrI12e%2F4wZNIwCtf7TRI%2BM%3D'\n",
    "if not os.path.exists('Train-28x28_cntk_text.txt') or not os.path.exists('Test-28x28_cntk_text.txt'):\n",
    "    utilities.download_file(mnist_dataset_url, 'mnist_dataset.zip')\n",
    "    print('Extracting MNIST dataset...')\n",
    "    with zipfile.ZipFile('mnist_dataset.zip', 'r') as z:\n",
    "        z.extractall('.')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create File Share and Upload MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_dataset_directory = 'mnist_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to create folders and upload files into Azure File Share - you can use [Azure Portal](https://ms.portal.azure.com), [Storage Explorer](http://storageexplorer.com/), [Azure CLI2](/azure-cli-extension) or Azure SDK for your preferable programming language.\n",
    "In this example we will use Azure SDK for python to copy files into file share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally on large datasets it is better to use the Azure CLI, with this command\n",
    "\n",
    "> AzCopy /Source:https://myaccount1.blob.core.windows.net/mycontainer/ /Dest:https://myaccount2.file.core.windows.net/myfileshare/ /SourceKey:key1 /DestKey:key2 /S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "service = FileService(storage_account_name, storage_account_key)\n",
    "service.create_directory(\n",
    "    azure_file_share_name, mnist_dataset_directory,\n",
    "    fail_on_exist=False)\n",
    "# Since uploading can take significant time, let's check first if the\n",
    "# file has been uploaded already.\n",
    "for f in ['Train-28x28_cntk_text.txt', 'Test-28x28_cntk_text.txt']:\n",
    "    if service.exists(azure_file_share_name, mnist_dataset_directory, f):\n",
    "        continue\n",
    "    service.create_file_from_path(\n",
    "        azure_file_share_name, mnist_dataset_directory, f, f)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Sample Script and Configure the Input Directories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each job we will create a folder containing a copy of [ConvNet_MNIST.py](/recipes/CNTK-GPU-Python/ConvNet_MNIST.py). This allows to run the same job with different scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Uncomment this if you want to use CNTK\n",
    "#cntk_script_path = \"cntk_samples\"\n",
    "#service = FileService(storage_account_name, storage_account_key)\n",
    "#service.create_directory(\n",
    "#    azure_file_share_name, cntk_script_path, fail_on_exist=False)\n",
    "#service.create_file_from_path(\n",
    "#    azure_file_share_name, cntk_script_path, 'ConvNet_MNIST.py', 'ConvNet_MNIST.py')\n",
    "\n",
    "mnist_script_directory = 'tensorflow_samples'\n",
    "service = FileService(storage_account_name, storage_account_key)\n",
    "service.create_directory(\n",
    "    azure_file_share_name, mnist_script_directory, fail_on_exist=False)\n",
    "service.create_file_from_path(\n",
    "    azure_file_share_name, mnist_script_directory, 'convolutional.py', 'convolutional.py')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job needs to know where to find ConvNet_MNIST.py and input MNIST dataset. We will create two input directories for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_directories = [\n",
    "#    models.InputDirectory(\n",
    "#        id='SCRIPT',\n",
    "#        path='$AZ_BATCHAI_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, cntk_script_path)),\n",
    "#    models.InputDirectory(\n",
    "#        id='DATASET',\n",
    "#        path='$AZ_BATCHAI_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, mnist_dataset_directory))]\n",
    "\n",
    "\n",
    "input_directories = [\n",
    "    models.InputDirectory(\n",
    "        id='SCRIPT',\n",
    "        path='$AZ_BATCHAI_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, mnist_script_directory))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job will be able to reference those directories using ```$AZ_BATCHAI_INPUT_SCRIPT``` and ```$AZ_BATCHAI_INPUT_DATASET``` environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Output Directories\n",
    "We will store standard and error output of the job in File Share:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_output_path_prefix = \"$AZ_BATCHAI_MOUNT_ROOT/{0}\".format(azure_file_share)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model output will be stored in File Share:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_directories = [\n",
    "    models.OutputDirectory(\n",
    "        id='MODEL',\n",
    "        path_prefix='$AZ_BATCHAI_MOUNT_ROOT/{0}'.format(azure_file_share),\n",
    "        path_suffix=\"Models\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job will be able to reference this directory as `$AZ_BATCHAI_OUTPUT_MODEL` and we will be able to enumerate files in this directory using `MODEL` id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Job\n",
    "- The job will use `microsoft/cntk:2.1-gpu-python3.5-cuda8.0-cudnn6.0` container.\n",
    "- Will use configured previously input and output directories;\n",
    "- Will run modified ConvNet_MNIST.py providing MNIST Dataset path as the first parameter and desired mode output as the second one.\n",
    "- By removing container_settings, the job will be ran on the host VMs if you are using DSVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_name = datetime.utcnow().strftime(\"cntk_%m_%d_%Y_%H%M%S\")\n",
    "parameters = models.job_create_parameters.JobCreateParameters(\n",
    "     location=location,\n",
    "     cluster=models.ResourceId(cluster.id),\n",
    "     node_count=nodes_count,\n",
    "     input_directories=input_directories,\n",
    "     std_out_err_path_prefix=std_output_path_prefix,\n",
    "     output_directories=output_directories,\n",
    "     #container_settings=models.ContainerSettings(\n",
    "     #    models.ImageSourceRegistry(image='microsoft/cntk:2.1-gpu-python3.5-cuda8.0-cudnn6.0')),\n",
    "     container_settings=models.ContainerSettings(\n",
    "         models.ImageSourceRegistry(image='tensorflow/tensorflow:1.1.0-gpu')),\n",
    "     #cntk_settings = models.CNTKsettings(\n",
    "     #    python_script_file_path='$AZ_BATCHAI_INPUT_SCRIPT/ConvNet_MNIST.py',\n",
    "     #    command_line_args='$AZ_BATCHAI_INPUT_DATASET $AZ_BATCHAI_OUTPUT_MODEL')\n",
    "     tensor_flow_settings=models.TensorFlowSettings(\n",
    "     python_script_file_path='$AZ_BATCHAI_INPUT_SCRIPT/convolutional.py',\n",
    "     master_command_line_args=\"-p\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training Job and wait for Job completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Job: cntk_02_13_2018_201759\n"
     ]
    }
   ],
   "source": [
    "job = client.jobs.create(resource_group, job_name, parameters).result()\n",
    "print('Created Job: {}'.format(job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Job to Finish\n",
    "The job will start running when the cluster will have enought idle nodes. The following code waits for job to start running printing the cluster state. During job run, the code prints current content of stdout.\n",
    "\n",
    "**Note** Execution may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: AllocationState.steady Target: 1; Allocated: 1; Idle: 1; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: succeeded ExitCode: 0\n",
      "Waiting for job output to become available...\n",
      "2018-02-13 20:23:03.711966: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-02-13 20:23:03.718305: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-02-13 20:23:03.722598: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-02-13 20:23:03.727599: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-02-13 20:23:03.733159: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-02-13 20:23:04.669500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \n",
      "name: Tesla K80\n",
      "major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n",
      "pciBusID 20ce:00:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 11.10GiB\n",
      "2018-02-13 20:23:04.674941: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x36e5060\n",
      "2018-02-13 20:23:05.386362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \n",
      "name: Tesla K80\n",
      "major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n",
      "pciBusID 3715:00:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 11.10GiB\n",
      "2018-02-13 20:23:05.392137: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x36e9590\n",
      "2018-02-13 20:23:06.071202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 2 with properties: \n",
      "name: Tesla K80\n",
      "major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n",
      "pciBusID 4f1f:00:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 11.10GiB\n",
      "2018-02-13 20:23:06.077664: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0x36edb00\n",
      "2018-02-13 20:23:06.759216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 3 with properties: \n",
      "name: Tesla K80\n",
      "major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n",
      "pciBusID 72ae:00:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 11.10GiB\n",
      "2018-02-13 20:23:06.765838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 1\n",
      "2018-02-13 20:23:06.770366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 2\n",
      "2018-02-13 20:23:06.776977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 0 and 3\n",
      "2018-02-13 20:23:06.781771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 0\n",
      "2018-02-13 20:23:06.787386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 2\n",
      "2018-02-13 20:23:06.793050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 1 and 3\n",
      "2018-02-13 20:23:06.797769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 2 and 0\n",
      "2018-02-13 20:23:06.804089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 2 and 1\n",
      "2018-02-13 20:23:06.809931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 2 and 3\n",
      "2018-02-13 20:23:06.815422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 3 and 0\n",
      "2018-02-13 20:23:06.822399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 3 and 1\n",
      "2018-02-13 20:23:06.828155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:779] Peer access not supported between device ordinals 3 and 2\n",
      "2018-02-13 20:23:06.834308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 2 3 \n",
      "2018-02-13 20:23:06.841069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y N N N \n",
      "2018-02-13 20:23:06.845522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   N Y N N \n",
      "2018-02-13 20:23:06.851403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 2:   N N Y N \n",
      "2018-02-13 20:23:06.857219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 3:   N N N Y \n",
      "2018-02-13 20:23:06.882079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 20ce:00:00.0)\n",
      "2018-02-13 20:23:06.887860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 3715:00:00.0)\n",
      "2018-02-13 20:23:06.893894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 4f1f:00:00.0)\n",
      "2018-02-13 20:23:06.899272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 72ae:00:00.0)\n",
      "Job state: succeeded ExitCode: 0\n"
     ]
    }
   ],
   "source": [
    "#utilities.wait_for_job_completion(client, resource_group, job_name, cluster_name, 'stdOuterr', 'stderr.txt')\n",
    "utilities.wait_for_job_completion(client, resource_group, job_name, cluster_name, 'stdOuterr', 'stderr-wk-0.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download stdout.txt and stderr.txt files for the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://batchaipablo.file.core.windows.net/batchaisample/b1395605-1fe9-4af4-b3ff-82a4725a3791/batchai_rg/jobs/cntk_02_13_2018_201759/6a021e0f-b1f3-44db-8b8a-9cd1ed4f0321/stderr-wk-0.txt?sv=2016-05-31&sr=f&sig=0RQ12oO%2FswOJmBf%2FfsNLxO%2FAbCe2AF0jZRX3cMKr3qA%3D&se=2018-02-13T21%3A32%3A15Z&sp=rl ...Done\n",
      "Downloading https://batchaipablo.file.core.windows.net/batchaisample/b1395605-1fe9-4af4-b3ff-82a4725a3791/batchai_rg/jobs/cntk_02_13_2018_201759/6a021e0f-b1f3-44db-8b8a-9cd1ed4f0321/stdout-wk-0.txt?sv=2016-05-31&sr=f&sig=EN8Q5K%2BTOnFjZsjRjynqSBeWVvudtiLSUKW4XH1x35s%3D&se=2018-02-13T21%3A32%3A15Z&sp=rl ...Done\n",
      "All files downloaded\n"
     ]
    }
   ],
   "source": [
    "files = client.jobs.list_output_files(resource_group, job_name, models.JobsListOutputFilesOptions(\"stdOuterr\")) \n",
    "for f in list(files):\n",
    "    utilities.download_file(f.download_url, f.name)\n",
    "print(\"All files downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enumerate Model Output\n",
    "Previously we configured the job to use output directory with `ID='MODEL'` for model output. We can enumerate the output using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CNTK only\n",
    "files = client.jobs.list_output_files(resource_group, job_name, models.JobsListOutputFilesOptions(\"MODEL\")) \n",
    "for f in list(files):\n",
    "    print(f.name, f.download_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.jobs.delete(resource_group, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Cluster\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = client.clusters.delete(resource_group, cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delete File Share\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the file share completely with all files using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = FileService(storage_account_name, storage_account_key)\n",
    "service.delete_share(azure_file_share_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
